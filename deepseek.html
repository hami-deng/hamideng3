<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek 爆火后的新闻报道数据分析</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f9f9f9;
            color: #333;
        }
        h1, h2 {
            color: #2c3e50;
        }
        .section {
            background: #fff;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        .section img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin-top: 10px;
        }
        .code {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            font-family: "Courier New", Courier, monospace;
            overflow-x: auto;
        }
        video {
            max-width: 100%;
            border-radius: 12px;
            margin-top: 30px;
        }
    </style>
</head>
<body>
    <h1>DeepSeek 爆火后的新闻报道数据分析</h1>

    <!-- DeepSeek 介绍 -->
    <div class="section">
        <h2>1. DeepSeek 介绍</h2>
        <p>
            DeepSeek，全称杭州深度求索人工智能基础技术研究有限公司，是一家创新型科技公司，成立于 2023 年 7 月 17 日。DeepSeek 使用数据蒸馏技术，从海量数据中提取出更为精炼、有用的信息，专注于开发先进的大语言模型（LLM）和相关技术。该公司由知名私募巨头幻方量化孕育而生，凭借其强大的技术背景和创新能力，迅速在人工智能领域崭露头角。
        </p>
        <p>
            2025 年 1 月 27 日，DeepSeek 应用登顶苹果美国地区应用商店免费 APP 下载排行榜，在美区下载榜上超越了 ChatGPT。同日，苹果中国区应用商店免费榜显示，DeepSeek 成为中国区第一。根据公开报道，DeepSeek 的员工规模不及 OpenAI 的 1/5，百人出头的公司中，算子、推理框架、多模态等研发工程师以及深度学习方面的研究人员共有约 70 人，主要在北京分部，其余 30 多人在杭州总部，多为前端、产品以及商务人员。
        </p>
    </div>

    <!-- 数据爬取 -->
    <div class="section">
        <h2>2. 数据爬取</h2>
        <p>
            为了分析 DeepSeek 爆火后的新闻报道数据，我利用 Python 的 Selenium 库编写了一个自动化程序，在火狐浏览器上通过百度搜索爬取相关新闻标题<strong>100条</strong>。以下是爬取过程的主要步骤：
        </p>
        <ol>
            <li><strong>环境搭建</strong>：安装必要的 Python 的 Selenium 库和 Firefox 浏览器驱动（GeckoDriver）。</li>
            <li><strong>配置浏览器和初始化</strong>:配置 Firefox 浏览器选项并初始化浏览器,例如无头模式、语言偏好等,自动下载和管理 GeckoDriver。</li>
            <li><strong>新闻标题数据爬取</strong>：在百度搜索页面上输入关键词“DeepSeek 最新消息”，使用 Selenium 定位搜索结果页面的新闻标题元素，提取每条新闻的标题。</li>
            <li><strong>自动翻页和保存</strong>：通过自动程序来点击“下一页”按钮爬取多页数据，将爬取的标题分批写入 CSV 文件。</li>
            <li><strong>日志记录和异常处理</strong>：使用 logging 模块记录程序运行日志，并捕获和处理异常。</li>
        </ol>
        <div class="code">
            <pre>
from selenium import webdriver
from selenium.webdriver import FirefoxOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.firefox import GeckoDriverManager
import csv
import time
import logging
import os

# 设置日志记录
logging.basicConfig(filename='debug.log', level=logging.DEBUG)

def main():
    driver = None  # 提前声明 driver 变量
    try:
        logging.info("正在配置浏览器选项...")
        print("正在配置浏览器选项...")

        # 配置浏览器
        opts = FirefoxOptions()
        # opts.add_argument("--headless")  # 调试时先关闭无头模式
        opts.set_preference("intl.accept_languages", "en-US")

        logging.info("正在初始化浏览器...")
        print("正在初始化浏览器...")

        # 初始化浏览器
        service = webdriver.firefox.service.Service(GeckoDriverManager().install())
        driver = webdriver.Firefox(service=service, options=opts)
        driver.implicitly_wait(10)

        logging.info("正在访问百度搜索页面...")
        print("正在访问百度搜索页面...")

        # 执行搜索
        search_query = "deepseek的最新相关消息"
        driver.get(f"https://www.baidu.com/s?wd={search_query}")

        logging.info("正在等待搜索结果加载...")
        print("正在等待搜索结果加载...")

        # 等待搜索结果加载
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.result.c-container"))
        )

        logging.info("正在解析文章数据...")
        print("正在解析文章数据...")

        # 解析数据
        titles = []
        max_titles = 100  # 目标标题数量
        page_count = 1  # 当前页码
        batch_size = 10  # 每批写入 10 条
        csv_file = "deepseek_news.csv"  # 输出文件名

        # 初始化 CSV 文件（如果文件不存在，写入表头）
        if not file_exists(csv_file):
            logging.info(f"文件 {csv_file} 不存在，正在创建并写入表头...")
            with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                writer.writerow(['标题'])  # 写入表头
        else:
            logging.info(f"文件 {csv_file} 已存在，将追加数据...")

        while len(titles) < max_titles:
            logging.info(f"正在解析第 {page_count} 页数据...")
            print(f"正在解析第 {page_count} 页数据...")

            results = driver.find_elements(By.CSS_SELECTOR, "div.result.c-container")
            for result in results:
                if len(titles) >= max_titles:
                    break
                try:
                    title_element = result.find_element(By.CSS_SELECTOR, "h3.t")
                    title = title_element.text
                    titles.append(title)
                    logging.info(f"标题: {title}")
                    print(f"标题: {title}")
                except Exception as e:
                    logging.error(f"解析标题时发生异常: {str(e)}")
                    print(f"解析标题时发生异常: {str(e)}")

            # 检查是否已达到目标数量
            if len(titles) >= max_titles:
                logging.info(f"已收集到 {len(titles)} 条标题，停止爬取...")
                print(f"已收集到 {len(titles)} 条标题，停止爬取...")
                break

            # 尝试翻页
            try:
                next_page_button = driver.find_element(By.XPATH, "//a[@class='n' and contains(text(), '下一页')]")
                if next_page_button:
                    logging.info("找到下一页按钮，正在点击...")
                    print("找到下一页按钮，正在点击...")
                    driver.execute_script("arguments[0].scrollIntoView();", next_page_button)  # 滚动到按钮位置
                    next_page_button.click()
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.result.c-container"))
                    )
                    page_count += 1
                    time.sleep(2)  # 添加适当的延迟
                else:
                    logging.info("没有找到下一页按钮，停止爬取...")
                    print("没有找到下一页按钮，停止爬取...")
                    break
            except Exception as e:
                logging.error(f"找不到下一页按钮或点击失败: {str(e)}")
                print(f"找不到下一页按钮或点击失败: {str(e)}")
                break

            # 每爬取一批数据就写入 CSV 文件
            if len(titles) % batch_size == 0:
                logging.info(f"正在将第 {len(titles) // batch_size} 批数据写入 CSV 文件...")
                print(f"正在将第 {len(titles) // batch_size} 批数据写入 CSV 文件...")
                write_to_csv(titles, csv_file, append=True)

        # 写入剩余的数据
        if len(titles) % batch_size != 0:
            logging.info(f"正在将剩余数据写入 CSV 文件...")
            print(f"正在将剩余数据写入 CSV 文件...")
            write_to_csv(titles, csv_file, append=True)

        logging.info(f"数据解析完成，共收集到 {len(titles)} 条标题")
        print(f"数据解析完成，共收集到 {len(titles)} 条标题")

    except Exception as e:
        logging.error(f"发生异常: {str(e)}")
        print(f"发生异常: {str(e)}")
        if driver is not None:
            logging.info("保存错误截图...")
            print("保存错误截图...")
            driver.save_screenshot("error.png")
    finally:
        if driver is not None:
            logging.info("正在关闭浏览器...")
            print("正在关闭浏览器...")
            driver.quit()

def file_exists(filename):
    """检查文件是否存在"""
    try:
        with open(filename, 'r'):
            return True
    except FileNotFoundError:
        return False

def write_to_csv(titles, filename, append=False):
    """将标题写入 CSV 文件"""
    mode = 'a' if append else 'w'
    with open(filename, mode=mode, newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        if not append and not file_exists(filename):
            writer.writerow(['标题'])  # 写入表头
        for title in titles:
            writer.writerow([title])
    logging.info(f"成功将 {len(titles)} 条标题写入 {filename}")

if __name__ == "__main__":
    logging.info("程序开始运行...")
    print("程序开始运行...")
    main()
    logging.info("程序结束")
    print("程序结束")
            </pre>
        </div>
        <!-- 添加视频 -->
        <h3>数据爬取演示视频</h3>
        <video controls>
            <source src="Deep seek.mp4" type="video/mp4">
            您的浏览器不支持视频标签。
        </video>
        <p>以上视频演示了使用 Selenium 爬取百度搜索结果中的新闻标题的过程。</p>
    </div>

     <!-- 词云图制作 -->
    <div class="section">
        <h2>3. 词云图制作</h2>
        <p>
            为了直观展示新闻标题中的关键词，我使用 Python 的 <code>jieba</code> 和 <code>wordcloud</code> 库制作了词云图<strong>得到新闻报道的关键词有“deepseek”“模型”“崛起”“超越”“人工智能”“美国”等</strong>。以下是制作过程的主要步骤：
        </p>
        <ol>
            <li><strong>数据清洗</strong>：对新闻标题进行分词，去掉标点符号和停用词。</li>
            <li><strong>生成词云</strong>：将分词后的结果拼接成一个字符串，作为词云图的输入。</li>
            <li><strong>保存和展示</strong>：将生成的词云图保存为 PNG 文件（<code>wordcloud.png</code>）。</li>
        </ol>
        <div class="code">
            <pre>
from wordcloud import WordCloud
import jieba
import matplotlib.pyplot as plt

# 读取数据
titles = pd.read_csv("deepseek_news.csv")["标题"].tolist()

# 分词
text = " ".join(jieba.lcut(" ".join(titles)))

# 生成词云
wordcloud = WordCloud(font_path="simhei.ttf", width=800, height=400, background_color="white").generate(text)

# 保存词云图
wordcloud.to_file("wordcloud.png")
            </pre>
        </div>
        <img src="词云图.png" alt="词云图">
    </div>

    <!-- 情感分析 -->
    <div class="section">
        <h2>4. 情感分析</h2>
        <p>
            为了分析新闻标题的情感倾向，我使用 Python 的 <code>SnowNLP</code> 库对每条新闻标题进行情感分析。<strong>得到情感分析统计结果:积极情感48条，消极情感25条，中性情感11条。</strong>以下是分析过程的主要步骤：
        </p>
        <ol>
            <li><strong>情感得分计算</strong>：使用 <code>SnowNLP</code> 计算每条新闻标题的情感得分。</li>
            <li><strong>情感分类</strong>：根据情感得分将新闻标题分类为“积极”、“中性”或“消极”。</li>
            <li><strong>统计结果</strong>：统计每种情感分类的数量，并生成饼图展示情感分布。</li>
        </ol>
        <div class="code">
            <pre>
from snownlp import SnowNLP

# 情感分析
sentiment_scores = [SnowNLP(title).sentiments for title in titles]

# 分类
sentiment_labels = ["积极" if score >= 0.6 else "消极" if score <= 0.4 else "中性" for score in sentiment_scores]

# 统计结果
sentiment_counts = pd.Series(sentiment_labels).value_counts()

# 生成饼图
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')
plt.title("情感分析结果")
plt.savefig("sentiment_pie_chart.png")
            </pre>
        </div>
        <img src="情感分析.png" alt="情感分析结果图">
        <img src="饼图.png" alt="情感分析饼图">
    </div>
</body>
</html>

   
